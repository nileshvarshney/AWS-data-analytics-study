# Amazon S3
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability

## Use Cases
- Build a data lake
- Run Cloud Native Applications
- Backup and Store Critical data
- Archive Data at Low Cost

## Amazon S3 storage classes
The Amazon S3 storage classes will allow you to reduce the cost of data storage and choose the right tier based on your data's characteristics. Storage classes ordered by frequent access:
1. Amazon S3 Standard
    - General Purpose
    - High durability (99.999999999%) and High availability (99.99%), 3+ AZ
2. Amazon S3 StandarIntelligent Tiering
    - High durability (99.999999999%) and High availability (99.9%), 3+ AZ
    - Small monthly monitoring and auto tiering fee
3. Amazon S3 Standard Infrequent Access(IA)
    - High durability (99.999999999%) and High availability (99.9%), 3+ AZ
4. Amazon S3 One zone Infrequent Access
    - High durability (99.999999999%) and High availability (99.5%), Single AZ
    - Good for secordary backup or data can recreate
5. Amazon S3 Glacier
    - High durability (99.999999999%), 3+ AZ
    - Low cost storage with retrivel cost
    - Retrieval Option
        - Expedited (1 to 5 minutes)
        - Standard (3 to 5 hours)
        - Bulk (5 to 12 hours)

## Managing the object lifecycle with Amazon S3 
One of the ways you can transition your objects stored in Amazon S3 is by using lifecycle management. A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects.

When you configure lifecycle management for an Amazon S3 object or group of objects, there are two types of actions: transition and expiration actions. Transition actions define when objects transition to another storage class. For example, you might choose to transition objects to Amazon S3 Standard-IA storage class 30 days after you created them, or archive objects to the Amazon S3 Glacier storage class one year after creating them. Expiration actions define when objects expire. Amazon S3 deletes expired objects on your behalf.

**Periodic logs**
If you upload periodic logs to an S3 bucket, your application might need them for a week or a month. After that, you might want to delete them.

**Change in data access frequency**
Some documents are frequently accessed for a limited period of time. After that, they are infrequently accessed. At some point, you might not need real-time access to them, but your organization or regulations might require you to archive them for a specific period. After that, you can delete them.

**Archival data**
You might upload some types of data to Amazon S3 primarily for archival purposes. For example, you might archive digital media, financial and healthcare records, raw genomics sequence data, and long-term database backups. Often, you must retain this archival data  for regulatory compliance. You can retain this data in low-cost archival storage for the period of time that your compliance requires, and then delete it when you are no longer required to store it. 

*With lifecycle configuration rules, you can tell Amazon S3 to transition objects to less expensive storage classes, or archive or delete them. For more information on additional considerations for transitioning objects to another storage class*

## AWS S3 Versioning
S3 Versioning feature to preserve, retrieve, and restore every version of every object stored in your buckets. Versioning-enabled buckets can help you recover objects from accidental deletion or overwrite. if you delete an object, Amazon S3 inserts a delete marker instead of removing the object permanently. By default, S3 Versioning is disabled on buckets, and you must explicitly enable it.

You enable and suspend versioning at the bucket level. After you version-enable a bucket, it can never return to an unversioned state. But you can suspend versioning on that bucket. Any file that is not versioned prior to enabling versioning will have version “null”

## S3 Cross Region Replication
Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can replicate objects to a single destination bucket or to multiple destination buckets. The destination buckets can be in different AWS Regions or within the same Region as the source bucket.

To automatically replicate new objects as they are written to the bucket use live replication, such as Same-Region Replication (SRR) or Cross-Region Replication (CRR). To replicate existing objects to a different bucket on demand, use S3 Batch Replication. 

To enable SRR or CRR, you add a replication configuration to your source bucket. The minimum configuration must provide the following:
- The destination bucket or buckets where you want Amazon S3 to replicate objects
- An AWS Identity and Access Management (IAM) role that Amazon S3 can assume to replicate objects on your behalf
- Must enable versioning (source and destination)
**Use cases:** compliance, lower latency access, replication across accounts

## AWS S3 Performance - Key Names - Historically
- When you had > 100 TPS (transaction per second), S3 performance could degrade.
- Behind the scene, each object goes to an S3 partition and for the best performance, we want the highest partition distribution
- In the exam, and historically, it was recommended to have random characters in front of your key name to optimise performance
- It was recommended never to use dates to prefix keys

*As of July 17 th 2018, we can scale up to 3500 RPS for PUT and 5500 RPS for GET for EACH PREFIX. This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance.*

## AWS S3 Performance
- Faster upload of large objects (>5GB), use multipart upload
- Use CloudFront to cache S3 objects around the world (improves read)
- S3 Transfer Acceleration (uses edge locations) just need to change the endpoint you write to, not the code.
- If using SSE KMS encryption, you may be limited to your AWS limits for KMS usage (~100s 1000s downloads / uploads per second)

## S3 Encryption for Objects
- SSE-S3: 
    - Must set header: “x amz server side encryption": "AES256"
    - Keys handled & managed by AWS S3
- SSE-KMS:
    - Keys handled & managed by KMS
    - Must set header: “x amz server side encryption": ” aws:kms
    - KMS Advantages: user control + audit trail
- SSE-C:
    - Keys fully managed by the customer outside of AWS
    - HTTPS must be used
    - Encryption key must provided in HTTP headers, for every HTTP request made
- Client Side Encryption
    - Clients must encrypt data themselves before sending to S3
    - Clients must decrypt data themselves when retrieving from S3
    - Customer fully manages the keys and encryption cycle
- Encyption in Trasit / flight uses SSL/TLS
- S3 exposes HTTP and HTTPS but HTTPS is recommended.

# S3 CORS (Cross-origin resource sharing)

CORS defines a way for client web application that are loaded in one domain to interact with other resources in different domain. To configure your bucket to allow cross-origin requests, you add a CORS configuration to the bucket. A CORS configuration is a document that defines rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) supported for each origin, and other operation-specific information.

# S3 Access Log
You can identify Amazon S3 requests using Amazon S3 access logs. This log can be analyze by Amazon Athena.

*Amazon recommend that you use AWS CloudTrail data events instead of Amazon S3 access logs. CloudTrail data events are easier to set up and contain more information.*

## S3 Security
**USed Based Policy:**
    - IAM Policies - Which API calls should be allowed for specific users from IAM Console
**Resource Based:**
    - Bucket Policies
        - Bucket wide rule from S3 console
        - allow cross account
    - Object Access Control List (ACL) - fine grain
    - Bucket Access Control List (ACL) - less common

## S3 Bucket Policies
- JSON Based Policies
    - Resources (buckets and object)
    - Actions (Set of API to allow or Deny)
    - Effect ( Allow/ Deny)
    - Principal ( The account or user to apply the policy to)
- Use S3 bucket for policy to:
    - Grant public access to the bucket
    - Force object to be encrypted at upload
    - Grant access to another account(Cross account)
- Networking
    - Support VPC Endpoint
- Logging and Audit
    - S3 access log can be stored in other S3 bucket
    - API call can be logged in AWS Cloud Trail
User Security
    - Multi Factor Authentication (MFA) can be required in versioned buckets to delete objects
    - Signed URLs that are valid for a limited period


# Glacier
 - Low cost storage normally used to archiving abd backups to retain data for longer terms.
 - It can be considered as an alternative to on-premise maganatic tape storage.
 - It is highly durable(99.999999999%)
 - It is low cost storage comes with retrivel cost
 - Items in Glacier is called Archive(up to 40TB). Archives are stored in Valults.
 - There are 3 Retrival Options
    - Expedited (1 to 5 minutes)
    - Standard (3 to 5 hours)
    - Bulk (5 to 12 hours) 

## Valult Policies and Vault Lock
- Vault is collection of archives.
- Each Vault has
    - One Vault access policy (Written in JSON) - Similar to S3 bucket policy
    - One Vault Lock Policy (Written in JSON) - Lock for regulatory and compliance requirement. Lock policies are immutable, it can never be changed
